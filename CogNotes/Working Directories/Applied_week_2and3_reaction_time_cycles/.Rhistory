if(sum(data_temp$correct=='true')/length(data_temp$correct)>0.70){
data_temp$correct_p<-mean(as.numeric(data_temp$correct=='true'))
#Turn RT into numeric variable
data_temp$rt<-as.numeric(data_temp$rt)
#Determine a cutoff for when response times are suspiciously long (e.g. 3 SDs above the mean)
cutoff<-mean(data_temp$rt)+3*sd(data_temp$rt)
#Filter data with too long RTs
data_temp<- subset(data_temp,rt<cutoff)
#If this is the first time around the loop, make the currently loaded data into the actual data
if(length(colnames(dataFS))==0){
dataFS=data_temp
rm(data_temp)
#counter to monitor included datasets
n_datasets<-n_datasets+1
}
#Bind loaded data with actual data
else {dataFS<-rbind(dataFS,data_temp)
rm(data_temp)
#counter to monitor included datasets
n_datasets<-n_datasets+1
}
}
}
}
}
#A variable to monitor how many points we keep
n_datapoints<-length(dataFS[,1])
#Make a variable that says if the condition was a Stroop condition or not (incongruency between word and face)
dataFS$stroop<-as.factor(dataFS$face!=dataFS$word)
#Limit the data to only correct answers. Response time for incorrect responses can be difficult to interpret.
#Note that this will make the data unsuitable for analyses on accuracy.
dataFS<- subset(dataFS,correct=='true')
#Turn id into factor - this can be used to monitor number of unique participants and to model random effects
dataFS$id <- dataFS$id %>% as.factor()
#Make a variable which has exp onset in hour and minutes of the day as decimal variable
dataFS$hour2<-dataFS$hour+(dataFS$minute)/60
library(tidyverse)
x <- dataFS %>% dplyr::select(where(is.numeric))
# Use GGally to plot correlation matrix of both individual questions and summaries
library(ggcorrplot)
# ggcorrplot colors the squares according to whether the correlation is positive or negative
ggcorrplot(cor(x))
rm(x)
library(ggplot2)
ggplot(dataFS,aes(x=hour2,y=rt))+geom_smooth()+geom_point()+labs(x='Time of day (hours)', y='Response time')
ggplot(dataFS,aes(x=hour2,y=correct_p))+geom_smooth()+geom_point()+labs(x='Time of day (hours)', y='Accuracy (percent)')
ggplot(dataFS,aes(x=hour2,y=mood))+geom_smooth(na.rm=TRUE)+geom_point()+labs(x='Time of day (hours)', y='Negative/Positive Mood')
ggplot(dataFS,aes(x=hour2,y=hunger))+geom_smooth(na.rm=TRUE)+geom_point()+labs(x='Time of day (hours)', y='HungrySatiated')
ggplot(dataFS,aes(x=hour2,y=fresh))+geom_smooth(na.rm=TRUE)+geom_point()+labs(x='Time of day (hours)', y='Tired/Fresh')
ggplot(dataFS,aes(x=H_since_Eat,y=hunger))+geom_smooth(na.rm=TRUE)+geom_point()+labs(x='Time since eating (hours)', y='Hungry/Satiated')
ggplot(dataFS,aes(x=H_since_Eat,y=mood))+geom_smooth(na.rm=TRUE)+geom_point()+labs(x='Time since eating (hours)', y='Negative/Positive Mood')
ggplot(dataFS,aes(x=hour2,y=mood,col=id))+geom_line() + stat_summary(aes(group = 1), geom = "point", fun.y = mean,shape = 17, size = 3)+labs(x='Time of day (hours)', y='Negative/Positive Mood')
library(lmerTest)
## Simple 2x2 model with stroop incongruency, face type and interaction and participants as random effects
modelFSrt1<-lmer(rt~stroop*face+(1|id),data=dataFS)
summary(modelFSrt1)
#Inclusive model
modelFSrt2<-lmer(rt~+stroop+face+hour2+trial_index+H_since_Sleep+H_since_Eat+mood+Gender+(1|id),data=dataFS)
summary(modelFSrt2)
#modelFSmood<-lmer(mood~H_since_Sleep*H_since_Eat+(1|id),data=dataFS)
#summary(modelFSmood)
#not repated mood measurements
modelFSfresh<-lmer(fresh~H_since_Sleep*H_since_Eat+(1|id),data=dataFS)
summary(modelFSfresh)
sapply(dataFS,class)
scatterplot<- ggplot(dataFS,aes(mood,fresh))+
geom_point(aes())+
geom_smooth(method=lm,linetype="dashed",color="darkred")+
theme(legend.position=c(0,1), legend.justification=c(0,1))
scatter_gender_reg<- ggplot(dataFS,aes(mood,fresh))+
geom_point()+
geom_smooth(method=lm,linetype="dashed")+
theme(legend.position=c(0,1), legend.justification=c(0,1))
xdensity <- ggplot(dataFS, aes(mood)) +
geom_density(alpha=.5) +
theme(legend.position = "none")
ydensity <- ggplot(dataFS, aes(fresh)) +
geom_density(alpha=.5) +
theme(legend.position = "none")
blankPlot <- ggplot()+geom_blank(aes(1,1))+
theme(plot.background = element_blank(),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.border = element_blank(),
panel.background = element_blank(),
axis.title.x = element_blank(),
axis.title.y = element_blank(),
axis.text.x = element_blank(),
axis.text.y = element_blank(),
axis.ticks = element_blank()
)
gridExtra::grid.arrange(xdensity, blankPlot, scatterplot, ydensity,ncol=2, nrow=2, widths=c(4, 1.4), heights=c(1.4, 4))
gridExtra::grid.arrange(xdensity, blankPlot, scatter_gender_reg, ydensity,ncol=2, nrow=2, widths=c(4, 1.4), heights=c(1.4, 4))
library(tidyverse)
library(lmerTest)
#Get FaceStroopData
datadir<-"/Users/au83183/Documents/talks/applied_cog_sci_BSc_2022/applied_cog_sci_2022_classes/class02_biological_cycles/FaceStroopExp_data"
#Find files
files<-list.files(datadir,pattern='^FaceStroop_.+?csv',full.names=TRUE)
#Prepare an empty data frame for the data (also removes old version)
dataFS<-data.frame()
#How many datasets were there
n_datasets_raw<-length(files)
#Prepare a variable to monitor how many datasets we keep
n_datasets<-0
#Prepare a variable to monitor how many points we originally had
n_datapoints_raw<-0
#Loop to go through all files in the list
for(iii in 1:n_datasets_raw){
#remove old loaded file to not risk importing it multiple times
if(exists('data_temp')) rm(data_temp)
#Load data
data_temp<-read.csv(files[iii])
#Take trials that have an RT (remove intertrial fixations)
data_temp<- subset(data_temp,rt!='null' & trial_type=='image-keyboard-response')
#Make a variable with the total number of loaded data points, to know how many have been discarded
n_datapoints_raw<-n_datapoints_raw+length(data_temp[,1])
#Select data where at least 20 trials have been completed
if(length(data_temp[,1])>20){
#Some early data did not have a hunger measurement. Add a hunger variable for them and make them NAs
if("hunger" %in% names(data_temp)){} else data_temp$hunger=NA
#Some very ugly code because of script bug (you are able to select more than one value for hours since sleep and eat).
#We split the string, convert it from a list and turn the first value into an integer (a number)
if(is.integer(data_temp$H_since_Eat)==FALSE){
data_temp$H_since_Eat<-as.integer(unlist(strsplit(data_temp$H_since_Eat[1],","))[1])
}
if(is.integer(data_temp$H_since_Sleep)==FALSE){
data_temp$H_since_Sleep<-as.integer(unlist(strsplit(data_temp$H_since_Sleep[1],","))[1])
}
#Make sure that data have the same number of columns
if(length(colnames(data_temp))==33){
#Only include dataset with more than 70 percent correct responses
if(sum(data_temp$correct=='true')/length(data_temp$correct)>0.70){
data_temp$correct_p<-mean(as.numeric(data_temp$correct=='true'))
#Turn RT into numeric variable
data_temp$rt<-as.numeric(data_temp$rt)
data_temp$rt_median<-median(data_temp$rt)
#Determine a cutoff for when response times are suspiciously long (e.g. 3 SDs above the mean)
cutoff<-mean(data_temp$rt)+3*sd(data_temp$rt)
#Filter data with too long RTs
data_temp<- subset(data_temp,rt<cutoff)
#If this is the first time around the loop, make the currently loaded data into the actual data
if(length(colnames(dataFS))==0){
dataFS=data_temp
rm(data_temp)
#counter to monitor included datasets
n_datasets<-n_datasets+1
}
#Bind loaded data with actual data
else {dataFS<-rbind(dataFS,data_temp)
rm(data_temp)
#counter to monitor included datasets
n_datasets<-n_datasets+1
}
}
}
}
}
#Get FaceStroopData
datadir<-"./FaceStroopExp_data"
library(tidyverse)
library(lmerTest)
#Get FaceStroopData
datadir<-"./FaceStroopExp_data"
#Find files
files<-list.files(datadir,pattern='^FaceStroop_.+?csv',full.names=TRUE)
#Prepare an empty data frame for the data (also removes old version)
dataFS<-data.frame()
#How many datasets were there
n_datasets_raw<-length(files)
#Prepare a variable to monitor how many datasets we keep
n_datasets<-0
#Prepare a variable to monitor how many points we originally had
n_datapoints_raw<-0
#Loop to go through all files in the list
for(iii in 1:n_datasets_raw){
#remove old loaded file to not risk importing it multiple times
if(exists('data_temp')) rm(data_temp)
#Load data
data_temp<-read.csv(files[iii])
#Take trials that have an RT (remove intertrial fixations)
data_temp<- subset(data_temp,rt!='null' & trial_type=='image-keyboard-response')
#Make a variable with the total number of loaded data points, to know how many have been discarded
n_datapoints_raw<-n_datapoints_raw+length(data_temp[,1])
#Select data where at least 20 trials have been completed
if(length(data_temp[,1])>20){
#Some early data did not have a hunger measurement. Add a hunger variable for them and make them NAs
if("hunger" %in% names(data_temp)){} else data_temp$hunger=NA
#Some very ugly code because of script bug (you are able to select more than one value for hours since sleep and eat).
#We split the string, convert it from a list and turn the first value into an integer (a number)
if(is.integer(data_temp$H_since_Eat)==FALSE){
data_temp$H_since_Eat<-as.integer(unlist(strsplit(data_temp$H_since_Eat[1],","))[1])
}
if(is.integer(data_temp$H_since_Sleep)==FALSE){
data_temp$H_since_Sleep<-as.integer(unlist(strsplit(data_temp$H_since_Sleep[1],","))[1])
}
#Make sure that data have the same number of columns
if(length(colnames(data_temp))==33){
#Only include dataset with more than 70 percent correct responses
if(sum(data_temp$correct=='true')/length(data_temp$correct)>0.70){
data_temp$correct_p<-mean(as.numeric(data_temp$correct=='true'))
#Turn RT into numeric variable
data_temp$rt<-as.numeric(data_temp$rt)
data_temp$rt_median<-median(data_temp$rt)
#Determine a cutoff for when response times are suspiciously long (e.g. 3 SDs above the mean)
cutoff<-mean(data_temp$rt)+3*sd(data_temp$rt)
#Filter data with too long RTs
data_temp<- subset(data_temp,rt<cutoff)
#If this is the first time around the loop, make the currently loaded data into the actual data
if(length(colnames(dataFS))==0){
dataFS=data_temp
rm(data_temp)
#counter to monitor included datasets
n_datasets<-n_datasets+1
}
#Bind loaded data with actual data
else {dataFS<-rbind(dataFS,data_temp)
rm(data_temp)
#counter to monitor included datasets
n_datasets<-n_datasets+1
}
}
}
}
}
#A variable to monitor how many points we keep
n_datapoints<-length(dataFS[,1])
#Make a variable that says if the condition was a Stroop condition or not (incongruency between word and face)
dataFS$stroop<-as.factor(dataFS$face!=dataFS$word)
#Limit the data to only correct answers. Response time for incorrect responses can be difficult to interpret.
#Note that this will make the data unsuitable for analyses on accuracy.
dataFS<- subset(dataFS,correct=='true')
#Turn id into factor - this can be used to monitor number of unique participants and to model random effects
dataFS$id <- dataFS$id %>% as.factor()
#Make a variable which has hour and minutes of the day as decimal variable
dataFS$hour2<-dataFS$hour+(dataFS$minute)/60
#Make a variable that says if the condition was a Stroop condition or not (incongruency between word and face)
dataFS$stroop<-as.factor(dataFS$face!=dataFS$word)
#Limit the data to only correct answers. Response time for incorrect responses can be difficult to interpret.
#Note that this will make the data unsuitable for analyses on accuracy.
dataFS<- subset(dataFS,correct=='true')
#Turn id into factor - this can be used to monitor number of unique participants and to model random effects
dataFS$id <- dataFS$id %>% as.factor()
#Make a variable which has hour and minutes of the day as decimal variable
dataFS$hour2<-dataFS$hour+(dataFS$minute)/60
library(tidyverse)
#Summarise data in order to make smaller dataset with only one row per experimental session.
dataFSs<-dataFS %>% group_by(exp_id) %>% slice_head()
#Include a mean response time
temp<-dataFS  %>% group_by(exp_id) %>% summarise(rt_mean=mean(rt))
#Merge rt_mean with data
dataFSs<-merge(dataFSs,temp, by='exp_id')
rm(temp)
#Radius
r<-1
# Get x-values from minus 1 to plus 1
x<-seq(-1,1,0.00001)
# A circle with c(0,0) centre can be written with these two equations (following Pythagoras)
y1<-sqrt(r^2-x^2)
y2<--sqrt(r^2-x^2)
y<-c(y1,y2)
x<-c(x,x)
#Plotting the circle with sine and cosine values
pp=pi/4
plot(x,y,type='l')
lines(x=c(0,0),y=c(-1,1))
lines(x=c(0,cos(pp)),y=c(0,sin(pp)),col='darkgreen')
lines(x=c(cos(pp),cos(pp)),y=c(0,sin(pp)),col='darkblue')
text(x=c(-0.15+cos(pp)),y=c(0.5*sin(pp)),labels='sin(x)',col='darkblue')
lines(x=c(0,cos(pp)),y=c(sin(pp),sin(pp)),col='darkred')
text(x=c(0.5*cos(pp)),y=c(+0.15+sin(pp)),labels='cos(x)',col='darkred')
#cycle frequency (in this case per hour) - one cycle per 24 hours
cf=1/24
#sample frequency per hour
fs=100
#Duration in hours
dur=24
#A time vector divided by fs
t = seq(0, dur, 1/fs)
#Make a sine wave (with amplitude =1) for each time point in t
u = sin(2*pi*cf*t)
#Make a cosine wave (with amplitude =1) for each time point in t
u2= cos(2*pi*cf*t)
#Plot the waves
plot(x=t,y=u, type='l',col='darkblue',xlab='time(hours)',ylab='Amplitude')
text(x=1+t[1000],y=-0.2+u[500],labels='y=sin(2*pi*cf*t)',col='darkblue')
lines(x=t,y=u2, type='l',col='darkred')
text(x=-1+t[500],y=-0.2+u2[500],labels='Y=cos(2*pi*cf*t)',col='darkred')
library(tidyverse)
library(lmerTest)
#Get FaceStroopData
datadir<-"./FaceStroopExp_data"
#Find files
files<-list.files(datadir,pattern='^FaceStroop_.+?csv',full.names=TRUE)
#Prepare an empty data frame for the data (also removes old version)
dataFS<-data.frame()
#How many datasets were there
n_datasets_raw<-length(files)
#Prepare a variable to monitor how many datasets we keep
n_datasets<-0
#Prepare a variable to monitor how many points we originally had
n_datapoints_raw<-0
#Loop to go through all files in the list
for(iii in 1:n_datasets_raw){
#remove old loaded file to not risk importing it multiple times
if(exists('data_temp')) rm(data_temp)
#Load data
data_temp<-read.csv(files[iii])
#Take trials that have an RT (remove intertrial fixations)
data_temp<- subset(data_temp,rt!='null' & trial_type=='image-keyboard-response')
#Make a variable with the total number of loaded data points, to know how many have been discarded
n_datapoints_raw<-n_datapoints_raw+length(data_temp[,1])
#Select data where at least 20 trials have been completed
if(length(data_temp[,1])>20){
#Some early data did not have a hunger measurement. Add a hunger variable for them and make them NAs
if("hunger" %in% names(data_temp)){} else data_temp$hunger=NA
#Some very ugly code because of script bug (you are able to select more than one value for hours since sleep and eat).
#We split the string, convert it from a list and turn the first value into an integer (a number)
if(is.integer(data_temp$H_since_Eat)==FALSE){
data_temp$H_since_Eat<-as.integer(unlist(strsplit(data_temp$H_since_Eat[1],","))[1])
}
if(is.integer(data_temp$H_since_Sleep)==FALSE){
data_temp$H_since_Sleep<-as.integer(unlist(strsplit(data_temp$H_since_Sleep[1],","))[1])
}
#Make sure that data have the same number of columns
if(length(colnames(data_temp))==33){
#Only include dataset with more than 70 percent correct responses
if(sum(data_temp$correct=='true')/length(data_temp$correct)>0.70){
data_temp$correct_p<-mean(as.numeric(data_temp$correct=='true'))
#Turn RT into numeric variable
data_temp$rt<-as.numeric(data_temp$rt)
data_temp$rt_median<-median(data_temp$rt)
#Determine a cutoff for when response times are suspiciously long (e.g. 3 SDs above the mean)
cutoff<-mean(data_temp$rt)+3*sd(data_temp$rt)
#Filter data with too long RTs
data_temp<- subset(data_temp,rt<cutoff)
#If this is the first time around the loop, make the currently loaded data into the actual data
if(length(colnames(dataFS))==0){
dataFS=data_temp
rm(data_temp)
#counter to monitor included datasets
n_datasets<-n_datasets+1
}
#Bind loaded data with actual data
else {dataFS<-rbind(dataFS,data_temp)
rm(data_temp)
#counter to monitor included datasets
n_datasets<-n_datasets+1
}
}
}
}
}
#A variable to monitor how many points we keep
n_datapoints<-length(dataFS[,1])
#Make a variable that says if the condition was a Stroop condition or not (incongruency between word and face)
dataFS$stroop<-as.factor(dataFS$face!=dataFS$word)
#Limit the data to only correct answers. Response time for incorrect responses can be difficult to interpret.
#Note that this will make the data unsuitable for analyses on accuracy.
dataFS<- subset(dataFS,correct=='true')
#Turn id into factor - this can be used to monitor number of unique participants and to model random effects
dataFS$id <- dataFS$id %>% as.factor()
#Make a variable which has hour and minutes of the day as decimal variable
dataFS$hour2<-dataFS$hour+(dataFS$minute)/60
library(tidyverse)
#Summarise data in order to make smaller dataset with only one row per experimental session.
dataFSs<-dataFS %>% group_by(exp_id) %>% slice_head()
#Include a mean response time
temp<-dataFS  %>% group_by(exp_id) %>% summarise(rt_mean=mean(rt))
#Merge rt_mean with data
dataFSs<-merge(dataFSs,temp, by='exp_id')
rm(temp)
#Radius
r<-1
# Get x-values from minus 1 to plus 1
x<-seq(-1,1,0.00001)
# A circle with c(0,0) centre can be written with these two equations (following Pythagoras)
y1<-sqrt(r^2-x^2)
y2<--sqrt(r^2-x^2)
y<-c(y1,y2)
x<-c(x,x)
#Plotting the circle with sine and cosine values
pp=pi/4
plot(x,y,type='l')
lines(x=c(0,0),y=c(-1,1))
lines(x=c(0,cos(pp)),y=c(0,sin(pp)),col='darkgreen')
lines(x=c(cos(pp),cos(pp)),y=c(0,sin(pp)),col='darkblue')
text(x=c(-0.15+cos(pp)),y=c(0.5*sin(pp)),labels='sin(x)',col='darkblue')
lines(x=c(0,cos(pp)),y=c(sin(pp),sin(pp)),col='darkred')
text(x=c(0.5*cos(pp)),y=c(+0.15+sin(pp)),labels='cos(x)',col='darkred')
#cycle frequency (in this case per hour) - one cycle per 24 hours
cf=1/24
#sample frequency per hour
fs=100
#Duration in hours
dur=24
#A time vector divided by fs
t = seq(0, dur, 1/fs)
#Make a sine wave (with amplitude =1) for each time point in t
u = sin(2*pi*cf*t)
#Make a cosine wave (with amplitude =1) for each time point in t
u2= cos(2*pi*cf*t)
#Plot the waves
plot(x=t,y=u, type='l',col='darkblue',xlab='time(hours)',ylab='Amplitude')
text(x=1+t[1000],y=-0.2+u[500],labels='y=sin(2*pi*cf*t)',col='darkblue')
lines(x=t,y=u2, type='l',col='darkred')
text(x=-1+t[500],y=-0.2+u2[500],labels='Y=cos(2*pi*cf*t)',col='darkred')
#cycle frequency (in this case per hour) - one cycle per 24 hours
cf=1/24
#sample frequency per hour
fs=100
#Duration in hours
dur=24
#A time vector divided by fs
t = seq(0, dur, 1/fs)
#a  phase shift of pi/2 radians (half a cycle)
phi=pi
#Make a sine wave (with amplitude =1)
u = sin(2*pi*cf*t)
#Make a sine wave (with amplitude =1), and phase shift
u2= sin(2*pi*cf*t+phi)
#Plot the waves
plot(x=t,y=u, type='l',col='darkblue',xlab='time(hours)',ylab='Amplitude')
text(x=1+t[1000],y=-0.2+u[500],labels='y=sin(2*pi*cf*t)',col='darkblue')
lines(x=t,y=u2, type='l',lty='dashed',col='darkblue')
text(x=-1+t[500],y=0.2+u2[500],labels='Y=sin(2*pi*cf*t+phi)',col='darkblue')
#Make sine and cosine waves for each time point present in the data
dataFS$sinCirc<-sin(2*pi*cf*dataFS$hour2)
dataFS$cosCirc<-cos(2*pi*cf*dataFS$hour2)
# Plot the predictors for each data point in the data
plot(x=dataFS$hour2,y=dataFS$sinCirc,type='p',col='darkblue')
lines(x=dataFS$hour2,y=dataFS$cosCirc,type='p',col='darkred')
# Simple linear model with time of day predicting RT
modelFSrtHour<-lmer(rt~hour2+(1|id),data=dataFS)
# 2nd order polynomial model with time-squared predicting RT
modelFSrtHour2<-lmer(rt~poly(hour2,2)+(1|id),data=dataFS)
# Oscillation model with a sine and a cosine function predicting RT
modelFSrtCirc<-lmer(rt~sinCirc+cosCirc+(1|id),data=dataFS)
#Comparing the three different models to see which best explains the data
anova(modelFSrtHour,modelFSrtHour2,modelFSrtCirc)
#Get info from model 1
m_temp<-summary(modelFSrtHour)
m_temp
#Make a variable with the fitted effect using the regression coefficients
dataFS$HourFit<-m_temp$coefficients[1,1]+m_temp$coefficients[2,1]*dataFS$hour2
#Get info from model 2
m_temp<-summary(modelFSrtCirc)
m_temp
#Make a variable with the fitted effect using the regression coefficients
dataFS$CircFit<-m_temp$coefficients[1,1]+m_temp$coefficients[2,1]*dataFS$sinCirc+m_temp$coefficients[3,1]*dataFS$cosCirc
#Get info from model 3
m_temp<-summary(modelFSrtHour2)
m_temp
#Make a variable with the fitted effect using the regression coefficients
dataFS$Hour2Fit<-m_temp$coefficients[1,1]+m_temp$coefficients[2,1]*dataFS$hour2+m_temp$coefficients[2,1]*dataFS$hour2^2
library(ggplot2)
#Make a fitted plot. Include a non-linear fit (geom_smooth) for comparison.
ggplot(dataFS,aes(x=hour2,y=rt))+geom_smooth()+geom_smooth(aes(x=hour2,y=HourFit),col='darkred')+geom_smooth(aes(x=hour2,y=CircFit),col='darkblue')+labs(x='Time of day (hours)', y='Response time')
# Freshness: Simple oscillation model
modelFSfreshCirc<-lmer(fresh~sinCirc+cosCirc+(1|id),data=dataFS)
#Get info from freshness model
m_temp<-summary(modelFSfreshCirc)
m_temp
#Make a variable with the fitted effect using the regression coefficients
dataFS$CircFreshFit<-m_temp$coefficients[1,1]+m_temp$coefficients[2,1]*dataFS$sinCirc+m_temp$coefficients[3,1]*dataFS$cosCirc
#Make a fitted plot. Include a non-linear fit (geom_smooth) for comparison.
ggplot(dataFS,aes(x=hour2,y=fresh))+geom_smooth()+geom_smooth(aes(x=hour2,y=CircFreshFit),col='darkgreen')+labs(x='Time of day (hours)', y='Tired/Fresh')
# Mood: Simple oscillation model
modelFSMoodCirc<-lmer(mood~sinCirc+cosCirc+(1|id),data=dataFS)
#Get info from mood model
m_temp<-summary(modelFSMoodCirc)
m_temp
#Make a variable with the fitted effect using the regression coefficients
dataFS$CircMoodFit<-m_temp$coefficients[1,1]+m_temp$coefficients[2,1]*dataFS$sinCirc+m_temp$coefficients[3,1]*dataFS$cosCirc
#Make a fitted plot. Include a non-linear fit (geom_smooth) for comparison.
ggplot(dataFS,aes(x=hour2,y=mood))+geom_smooth()+geom_smooth(aes(x=hour2,y=CircMoodFit),col='darkred')+labs(x='Time of day (hours)', y='Negative/Positive Mood')
#Make a fitted plot. Include a non-linear fit (geom_smooth) for comparison.
ggplot(dataFS,aes(x=hour2,y=fresh))+geom_smooth(aes(x=hour2,y=scale(CircFit)),col='darkblue')+geom_smooth(aes(x=hour2,y=scale(CircFreshFit)),col='darkgreen')+geom_smooth(aes(x=hour2,y=scale(CircMoodFit)),col='darkred')+labs(x='Time of day (hours)', y='RT/Fresh/Mood')
View(dataFS)
# Simple linear model with time of day predicting RT
modelFSrtHour<-lmer(rt~hour2+(1|id),data=dataFS)
# 2nd order polynomial model with time-squared predicting RT
modelFSrtHour2<-lmer(rt~poly(hour2,2)+(1|id),data=dataFS)
# Oscillation model with a sine and a cosine function predicting RT
modelFSrtCirc<-lmer(rt~sinCirc+cosCirc+(1|id),data=dataFS)
#Comparing the three different models to see which best explains the data
anova(modelFSrtHour,modelFSrtHour2,modelFSrtCirc)
#Make a fitted plot. Include a non-linear fit (geom_smooth) for comparison.
ggplot(dataFS,aes(x=hour2,y=rt))+geom_smooth()+geom_smooth(aes(x=hour2,y=HourFit),col='darkred')+geom_smooth(aes(x=hour2,y=CircFit),col='darkgreen')+labs(x='Time of day (hours)', y='Response time')
# Freshness: Simple oscillation model
modelFSfreshCirc<-lmer(fresh~sinCirc+cosCirc+(1|id),data=dataFS)
#Get info from freshness model
m_temp<-summary(modelFSfreshCirc)
m_temp
#Make a variable with the fitted effect using the regression coefficients
dataFS$CircFreshFit<-m_temp$coefficients[1,1]+m_temp$coefficients[2,1]*dataFS$sinCirc+m_temp$coefficients[3,1]*dataFS$cosCirc
#Make a fitted plot. Include a non-linear fit (geom_smooth) for comparison.
ggplot(dataFS,aes(x=hour2,y=fresh))+geom_smooth()+geom_smooth(aes(x=hour2,y=CircFreshFit),col='darkgreen')+labs(x='Time of day (hours)', y='Tired/Fresh')
View(m_temp)
# Mood: Simple oscillation model
modelFSMoodCirc<-lmer(mood~sinCirc+cosCirc+(1|id),data=dataFS)
#Get info from mood model
m_temp<-summary(modelFSMoodCirc)
m_temp
#Make a variable with the fitted effect using the regression coefficients
dataFS$CircMoodFit<-m_temp$coefficients[1,1]+m_temp$coefficients[2,1]*dataFS$sinCirc+m_temp$coefficients[3,1]*dataFS$cosCirc
#Make a fitted plot. Include a non-linear fit (geom_smooth) for comparison.
ggplot(dataFS,aes(x=hour2,y=mood))+geom_smooth()+geom_smooth(aes(x=hour2,y=CircMoodFit),col='darkred')+labs(x='Time of day (hours)', y='Negative/Positive Mood')
#Make a fitted plot. Include a non-linear fit (geom_smooth) for comparison.
ggplot(dataFS,aes(x=hour2,y=fresh))+geom_smooth(aes(x=hour2,y=scale(CircFit)),col='darkblue')+geom_smooth(aes(x=hour2,y=scale(CircFreshFit)),col='darkgreen')+geom_smooth(aes(x=hour2,y=scale(CircMoodFit)),col='darkred')+labs(x='Time of day (hours)', y='RT/Fresh/Mood')
#Make a fitted plot. Include a non-linear fit (geom_smooth) for comparison.
ggplot(dataFS,aes(x=hour2,y=fresh))+geom_smooth(aes(x=hour2,y=scale(CircFit)),col='darkblue')+geom_smooth(aes(x=hour2,y=scale(CircFreshFit)),col='darkgreen')+geom_smooth(aes(x=hour2,y=scale(CircMoodFit)),col='darkred')+labs(x='Time of day (hours)', y='RT/Fresh/Mood')

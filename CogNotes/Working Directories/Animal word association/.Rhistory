geom_smooth(method = lm, se = FALSE)+
geom_text_repel()
laurits %>%
ggplot(aes(...1, reaction_time, label=word, color = reaction_time)) +
geom_line(color = "red") +
geom_text_repel()
laurits_dist %>%
ggplot(aes(...1, sem_dist, label=word, color = sem_dist)) +
geom_line(color = "red") +
geom_text_repel()
?tally()
df %>%
ggplot(aes(sem_dist, reaction_time, label = word, color = reaction_time)) +
geom_point() +
geom_smooth(method = lm, color = "red") +
geom_text_repel() +
labs(x = "semantic distance", y = "reaction time")
laurits_dist%>%
ggplot(aes(sem_dist, reaction_time, label = word, color = reaction_time)) +
geom_point() +
geom_smooth(method = lm, color = "red") +
geom_text_repel() +
labs(x = "semantic distance", y = "reaction time")
summary(lm(reaction_time ~ sem_dist, laurits_dist))
knitr::opts_chunk$set(echo = TRUE)
# import/load packages
# LSAfun is used for the semantic analysis. We will load a pretrained semantic model to get semantic distances between each animal name. See more documentation here: https://cran.r-project.org/web/packages/LSAfun/LSAfun.pdf
# ggrepel is a package that allow us to show text in ggplot without it overlapping with the data points (it provides an improved alternative to geom_text())
# ape is a package that we can use to make fancy dendogram network plots
pacman::p_load(tidyverse, LSAfun, ggrepel, ape, plyr, dplyr, ggplot2)
# Load them all in
data <- ldply(temp, read_csv)
# first we make a cross table with all pairwise semantic distances
df2 <- data.frame(matrix(ncol=nrow(laurits),nrow=nrow(laurits)))
View(df2)
colnames(df2) <- df$word
colnames(df2) <- df$word
colnames(df2) <- laurits$word
rownames(df2) <- laurits$word
View(df2)
summary(lm(reaction_time ~ sem_dist, laurits_dist))
# first we make a cross table with all pairwise semantic distances
df2 <- data.frame(matrix(ncol=nrow(laurits),nrow=nrow(laurits)))
colnames(df2) <- laurits$word
rownames(df2) <- laurits$word
df2[r,c] <- distance(rownames(df2)[r],rownames(df2)[c],method="euclidean",tvectors=EN_100k)
# notice that here we use loops in r :-). The syntax is a bit more confusing than in python, but it is the same thing
for (r in 1:length(df$sem_dist)){
for (c in 1:length(df$sem_dist)){
df2[r,c] <- distance(rownames(df2)[r],rownames(df2)[c],method="euclidean",tvectors=EN_100k)
}
}
# first we make a cross table with all pairwise semantic distances
df2 <- data.frame(matrix(ncol=nrow(laurits),nrow=nrow(laurits)))
colnames(df2) <- laurits$word
rownames(df2) <- laurits$word
# notice that here we use loops in r :-). The syntax is a bit more confusing than in python, but it is the same thing
for (r in 1:length(df$sem_dist)){
for (c in 1:length(df$sem_dist)){
df2[r,c] <- distance(rownames(df2)[r],rownames(df2)[c],method="euclidean",tvectors=EN_100k)
}
}
for (c in 1:length(laurits$sem_dist)){
df2[r,c] <- distance(rownames(df2)[r],rownames(df2)[c],method="euclidean",tvectors=EN_100k)
}
# notice that here we use loops in r :-). The syntax is a bit more confusing than in python, but it is the same thing
for (r in 1:length(laurits$sem_dist)){
for (c in 1:length(laurits$sem_dist)){
df2[r,c] <- distance(rownames(df2)[r],rownames(df2)[c],method="euclidean",tvectors=EN_100k)
}
}
# first we make a cross table with all pairwise semantic distances
df2 <- data.frame(matrix(ncol=nrow(laurits),nrow=nrow(laurits)))
colnames(df2) <- laurits$word
rownames(df2) <- laurits$word
# notice that here we use loops in r :-). The syntax is a bit more confusing than in python, but it is the same thing
for (r in 1:length(laurits$sem_dist)){
for (c in 1:length(laurits$sem_dist)){
df2[r,c] <- distance(rownames(df2)[r],rownames(df2)[c],method="euclidean",tvectors=EN_100k)
}
}
View(laurits_dist)
View(df2)
# notice that here we use loops in r :-). The syntax is a bit more confusing than in python, but it is the same thing
for (r in 1:length(laurits$sem_dist)){
for (c in 1:length(laurits$sem_dist)){
df2[r,c] <- distance(rownames(df2)[r],rownames(df2)[c],method="euclidean",tvectors=EN_100k)
}
}
df2[r,c] <- distance(rownames(df2)[r],rownames(df2)[c],method="euclidean",tvectors=EN_100k)
for (c in 1:length(laurits$sem_dist)){
df2[r,c] <- distance(rownames(df2)[r],rownames(df2)[c],method="euclidean",tvectors=EN_100k)
}
# notice that here we use loops in r :-). The syntax is a bit more confusing than in python, but it is the same thing
for (r in 1:length(laurits$sem_dist)){
for (c in 1:length(laurits$sem_dist)){
df2[r,c] <- distance(rownames(df2)[r],rownames(df2)[c],method="euclidean",tvectors=EN_100k)
}
}
# now we use some unsupervised clustering on the semantic distances to explore how the animals cluster together as a function of their semantic distance
dd <- dist(scale(df2), method = "euclidean")
hc <- hclust(dd, method = "ward.D2")
# first we make a cross table with all pairwise semantic distances
df2 <- data.frame(matrix(ncol=nrow(laurits),nrow=nrow(laurits)))
colnames(df2) <- laurits$word
rownames(df2) <- laurits$word
# notice that here we use loops in r :-). The syntax is a bit more confusing than in python, but it is the same thing
for (r in 1:length(laurits$sem_dist)){
for (c in 1:length(laurits$sem_dist)){
df2[r,c] <- distance(rownames(df2)[r],rownames(df2)[c],method="euclidean",tvectors=EN_100k)
}
}
df2[r,c] <- distance(rownames(df2)[r],rownames(df2)[c], method="euclidean", tvectors=EN_100k)
# first we make a cross table with all pairwise semantic distances
df2 <- data.frame(matrix(ncol=nrow(laurits),nrow=nrow(laurits)))
colnames(df2) <- laurits$word
rownames(df2) <- laurits$word
# notice that here we use loops in r :-). The syntax is a bit more confusing than in python, but it is the same thing
for (r in 1:length(laurits$sem_dist)){
for (c in 1:length(laurits$sem_dist)){
df2[r,c] <- distance(rownames(df2)[r],rownames(df2)[c], method="euclidean", tvectors=EN_100k)
}
}
for (c in 1:length(laurits_dist$sem_dist)){
df2[r,c] <- distance(rownames(df2)[r],rownames(df2)[c], method="euclidean", tvectors=EN_100k)
}
# notice that here we use loops in r :-). The syntax is a bit more confusing than in python, but it is the same thing
for (r in 1:length(laurits_dist$sem_dist)){
for (c in 1:length(laurits_dist$sem_dist)){
df2[r,c] <- distance(rownames(df2)[r],rownames(df2)[c], method="euclidean", tvectors=EN_100k)
}
}
# first we make a cross table with all pairwise semantic distances
df2 <- data.frame(matrix(ncol=nrow(laurits_dist),nrow=nrow(laurits_dist)))
colnames(df2) <- laurits_dist$word
rownames(df2) <- laurits_dist$word
# notice that here we use loops in r :-). The syntax is a bit more confusing than in python, but it is the same thing
for (r in 1:length(laurits_dist$sem_dist)){
for (c in 1:length(laurits_dist$sem_dist)){
df2[r,c] <- distance(rownames(df2)[r],rownames(df2)[c], method="euclidean", tvectors=EN_100k)
}
}
# now we use some unsupervised clustering on the semantic distances to explore how the animals cluster together as a function of their semantic distance
dd <- dist(scale(df2), method = "euclidean")
hc <- hclust(dd, method = "ward.D2")
# and then we can plot the result as a small network
plot(as.phylo(hc), type = "unrooted")
# and then we can plot the result as a small network
plot(as.phylo(hc), type = "rooted")
# and then we can plot the result as a small network
plot(as.phylo(hc), type = "unrooted")
freq_data %>%
filter(sem_dist != 3) %>%
ggplot(aes(sem_dist, color = sem_dist)) +
geom_histogram(binwidth = 0.3, aes(fill = ..count..)) +
scale_fill_gradient("count", low="orange", high="purple") +
labs(x = "Distance",
y = "Count",
title = "Semantic distance distribution") +
scale_x_continuous(breaks = seq(0,2,0.1)) +
theme_minimal()
laurits_dist %>%
filter(sem_dist != 3) %>%
ggplot(aes(sem_dist, color = sem_dist)) +
geom_histogram(binwidth = 0.3, aes(fill = ..count..)) +
scale_fill_gradient("count", low="orange", high="purple") +
labs(x = "Distance",
y = "Count",
title = "Semantic distance distribution") +
scale_x_continuous(breaks = seq(0,2,0.1)) +
theme_minimal()
laurits_dist %>%
filter(sem_dist != 3) %>%
ggplot(aes(sem_dist, color = sem_dist)) +
geom_histogram(binwidth = 0.1, aes(fill = ..count..)) +
scale_fill_gradient("count", low="orange", high="purple") +
labs(x = "Distance",
y = "Count",
title = "Semantic distance distribution") +
scale_x_continuous(breaks = seq(0,2,0.1)) +
theme_minimal()
laurits_dist %>%
filter(sem_dist != 3) %>%
ggplot(aes(sem_dist, color = sem_dist)) +
geom_histogram(binwidth = 1, aes(fill = ..count..)) +
scale_fill_gradient("count", low="orange", high="purple") +
labs(x = "Distance",
y = "Count",
title = "Semantic distance distribution") +
scale_x_continuous(breaks = seq(0,2,0.1)) +
theme_minimal()
laurits_dist %>%
filter(sem_dist != 3) %>%
ggplot(aes(sem_dist, color = sem_dist)) +
geom_histogram(binwidth = 2, aes(fill = ..count..)) +
scale_fill_gradient("count", low="orange", high="purple") +
labs(x = "Distance",
y = "Count",
title = "Semantic distance distribution") +
scale_x_continuous(breaks = seq(0,2,0.1)) +
theme_minimal()
View(laurits_dist)
laurits_dist %>%
filter(sem_dist != 3) %>%
ggplot(aes(sem_dist, color = sem_dist)) +
geom_histogram(binwidth = 2, aes(fill = ..count..)) +
scale_fill_gradient("count", low="orange", high="purple") +
labs(x = "Distance",
y = "Count",
title = "Semantic distance distribution") +
scale_x_continuous(breaks = seq(0,2,0.1)) +
theme_minimal()
laurits_dist %>%
ggplot(aes(sem_dist, color = sem_dist)) +
geom_histogram(binwidth = 2, aes(fill = ..count..)) +
scale_fill_gradient("count", low="orange", high="purple") +
labs(x = "Distance",
y = "Count",
title = "Semantic distance distribution") +
scale_x_continuous(breaks = seq(0,2,0.1)) +
theme_minimal()
laurits_dist %>%
filter(sem_dist != 3) %>%
ggplot(aes(sem_dist, color = sem_dist)) +
geom_histogram(binwidth = 0.1, aes(fill = ..count..)) +
scale_fill_gradient("count", low="orange", high="purple") +
labs(x = "Distance",
y = "Count",
title = "Semantic distance distribution") +
scale_x_continuous(breaks = seq(0,2,0.1)) +
theme_minimal()
laurits_dist %>%
filter(sem_dist != 3) %>%
ggplot(aes(sem_dist, color = sem_dist)) +
geom_histogram(binwidth = 5, aes(fill = ..count..)) +
scale_fill_gradient("count", low="orange", high="purple") +
labs(x = "Distance",
y = "Count",
title = "Semantic distance distribution") +
scale_x_continuous(breaks = seq(0,2,0.1)) +
theme_minimal()
pacman::p_load(tidyverse, LSAfun, ggrepel, ape, plyr, dplyr, ggplot2)
# Load the semantic model which is trained on 2 Billion word corpus, which consist of the British National Corpus (BNC), the ukWaC corpus and a 2009 Wikipedia dump
load("EN_100k.rda")
# Here you can play with the function that will return the semantic distance between two words
distance("garden", "flower", method="euclidean", tvectors=EN_100k)
# Here you can play with the function that will return the semantic distance between two words
distance("garden", "lawnmover", method="euclidean", tvectors=EN_100k)
# Here you can play with the function that will return the semantic distance between two words
distance("garden", "rake", method="euclidean", tvectors=EN_100k)
# Here you can play with the function that will return the semantic distance between two words
distance("garden", "tree", method="euclidean", tvectors=EN_100k)
# Here you can play with the function that will return the semantic distance between two words
distance("garden", "flower", method="euclidean", tvectors=EN_100k)
# Here you can play with the function that will return the semantic distance between two words
distance("garden", "bush", method="euclidean", tvectors=EN_100k)
# Here you can play with the function that will return the semantic distance between two words
distance("garden", "tree", method="euclidean", tvectors=EN_100k)
# Here you can play with the function that will return the semantic distance between two words
distance("garden", "grass", method="euclidean", tvectors=EN_100k)
# Here you can play with the function that will return the semantic distance between two words
distance("garden", "tree", method="euclidean", tvectors=EN_100k)
# Here you can play with the function that will return the semantic distance between two words
distance("garden", "lawnmower", method="euclidean", tvectors=EN_100k)
# Here you can play with the function that will return the semantic distance between two words
distance("garden", "rake", method="euclidean", tvectors=EN_100k)
# Here you can play with the function that will return the semantic distance between two words
distance("garden", "spade", method="euclidean", tvectors=EN_100k)
# Here you can play with the function that will return the semantic distance between two words
distance("garden", "shovel", method="euclidean", tvectors=EN_100k)
# Here you can play with the function that will return the semantic distance between two words
distance("garden", "house", method="euclidean", tvectors=EN_100k)
# Here you can play with the function that will return the semantic distance between two words
distance("garden", "flagpole", method="euclidean", tvectors=EN_100k)
# Here you can play with the function that will return the semantic distance between two words
distance("garden", "Frisbee", method="euclidean", tvectors=EN_100k)
# Here you can play with the function that will return the semantic distance between two words
distance("garden", "Spade", method="euclidean", tvectors=EN_100k)
# Here you can play with the function that will return the semantic distance between two words
distance("garden", "spade", method="euclidean", tvectors=EN_100k)
# Here you can play with the function that will return the semantic distance between two words
distance("garden", "tomato", method="euclidean", tvectors=EN_100k)
# Here you can play with the function that will return the semantic distance between two words
distance("garden", "cucumber", method="euclidean", tvectors=EN_100k)
# Here you can play with the function that will return the semantic distance between two words
distance("garden", "carrot", method="euclidean", tvectors=EN_100k)
# Here you can play with the function that will return the semantic distance between two words
distance("garden", "apple", method="euclidean", tvectors=EN_100k)
# Here you can play with the function that will return the semantic distance between two words
distance("garden", "tomato", method="euclidean", tvectors=EN_100k)
# Here you can play with the function that will return the semantic distance between two words
distance("garden", "potato", method="euclidean", tvectors=EN_100k)
# Here you can play with the function that will return the semantic distance between two words
distance("garden", "tomato", method="euclidean", tvectors=EN_100k)
# Here you can play with the function that will return the semantic distance between two words
distance("garden", "tree", method="euclidean", tvectors=EN_100k)
?rbinom()
#Let's try to simulate the data
#number of days in the experiment
days<-365
#birth rate per day for the two hospitals
n_births_small<-c(15)
n_births_large<-c(45)
# true distribution of boys
p_boy_t<-0.5
#The cut-off for days that we want to count as significant
cut_off<-0.6
fun_p_boys_obs<-function(n_births,n_days,p_boy_true){
boys<-rbinom(n_days, n_births,p_boy_true)
p_boys_obs<-boys/n_births
return(p_boys_obs)
}
#Use the function on the small hospital
small_hosp<-fun_p_boys_obs(n_births_small,days,p_boy_t)
large_hosp<-fun_p_boys_obs(n_births_large,days,p_boy_t)
#Use the function on the small hospital
small_hosp<-fun_p_boys_obs(n_births_small,days,p_boy_t)
sum(small_hosp>0.6)
#Use the function on the small hospital
small_hosp<-fun_p_boys_obs(n_births_small,days,p_boy_t)
sum(small_hosp>0.6)
#Use the function on the small hospital
small_hosp<-fun_p_boys_obs(n_births_small,days,p_boy_t)
large_hosp<-fun_p_boys_obs(n_births_large,days,p_boy_t)
library(kableExtra)
pacman:pload(kableExtra)
pacman::pload(kableExtra)
pacman::p_load(kableExtra)
pacman::p_load(kableExtra)
library(kableExtra)
door1<-c('goat','goat','car')
door2<-c('goat','car','goat')
door3<-c('car','goat','goat')
df<-data.frame(door1,door2,door3)
df<-data.frame(door1,door2,door3)
kable(df) %>% kable_classic(full_width = F, html_font = "Cambria")
library(ggplot2)
library(ggforce)
#Let's try to simulate the data
#number of days in the experiment
days<-365
#birth rate per day for the two hospitals
n_births_small<-c(15)
n_births_large<-c(45)
# true distribution of boys
p_boy_t<-0.5
#The cut-off for days that we want to count as significant
cut_off<-0.6
#Make a function to simulate the data
fun_p_boys_obs<-function(n_births,n_days,p_boy_true){ #defining a function, and the {} means that everything inside those are inside the function
boys<-rbinom(n_days, n_births,p_boy_true) #defining how boy-births are distributed, saying that the function should use a randomly binomial distribution
p_boys_obs<-boys/n_births #defining what the true distribution of boys is = boys/number of births
return(p_boys_obs) #feeds the result back to us
}
#Use the function on the small hospital
small_hosp<-fun_p_boys_obs(n_births_small,days,p_boy_t)
sum(small_hosp>0.6) #Inserts data into the function, and calculates how many days the amount of boys born are more than 60%
large_hosp<-fun_p_boys_obs(n_births_large,days,p_boy_t)
sum(large_hosp>0.6) #Inserts data into the function, and calculates how many days the amount of boys born are more than 60%
#One child per day hospital
mini_hosp<-fun_p_boys_obs(1,days,p_boy_t)
sum(mini_hosp>0.6)
library(ggplot2)
library(ggforce)
groups<-c('artists','beekepers','chemists')
X<-c(9,9,9)
Y<-c(9,4,5)
circle_size<-c(2,3,4)
Col=c('darkblue','darkgreen','darkred')
df<-data.frame(groups, X,Y,circle_size,Col)
figure<-ggplot(df)+
geom_circle(aes(x0=X,y0=Y,r=circle_size,fill=groups,col=groups),alpha=0.4)+
geom_text(aes(x=X,y=Y,label=groups,col=groups))+
theme_classic()+
scale_color_manual(values = Col)+
scale_fill_manual(values = Col)+
guides(fill='none',col='none')+
xlim(0, 15) + ylim(0,15)
figure
library(ggplot2)
library(ggforce)
groups<-c('doctors','nurses','women')
X<-c(9,9,9)
Y<-c(9,4,5)
circle_size<-c(2,3,4)
Col=c('darkblue','darkgreen','darkred')
df<-data.frame(groups, X,Y,circle_size,Col)
figure<-ggplot(df)+
geom_circle(aes(x0=X,y0=Y,r=circle_size,fill=groups,col=groups),alpha=0.4)+
geom_text(aes(x=X,y=Y,label=groups,col=groups))+
theme_classic()+
scale_color_manual(values = Col)+
scale_fill_manual(values = Col)+
guides(fill='none',col='none')+
xlim(0, 15) + ylim(0,15)
figure
library(ggplot2)
library(ggforce)
groups<-c('S','P')
X<-c(9,9)
Y<-c(4.5,4)
circle_size<-c(2,3)
Col=c('darkblue','darkgreen')
df<-data.frame(groups, X,Y,circle_size,Col)
figure<-ggplot(df)+
geom_circle(aes(x0=X,y0=Y,r=circle_size,fill=groups,col=groups),alpha=0.4)+
geom_text(aes(x=X,y=Y,label=groups,col=groups))+
theme_classic()+
scale_color_manual(values = Col)+
scale_fill_manual(values = Col)+
guides(fill='none',col='none')+
xlim(0, 15) + ylim(0,15)
figure
# Possible outcome
ifpthenq<-function(p,q){
if (p==TRUE & q==FALSE){out='This is false. P entails q, yet q is false while p is true.'}
if (p==TRUE & q==TRUE){out='True. P entails q. This is modus ponens.'}
if (p==FALSE & q==FALSE){out='True. If P entails q, then non-q entails non-P. This is modus tollens.'}
if (p==FALSE & q==TRUE){out='This is possible, but does not follow logically. P entails q, but q does not necessarily depend on p.'}
return(out)  }
P=TRUE
Q=TRUE
ifpthenq(P,Q)
library(kableExtra)
door1<-c('goat','goat','car')
door2<-c('goat','car','goat')
door3<-c('car','goat','goat')
df<-data.frame(door1,door2,door3)
kable(df) %>% kable_classic(full_width = F, html_font = "Cambria")
pacman::p_load(tidyberse,lme4,lmerTest)
#make sure lme4 is loaded
pacman::p_load(lme4)
#import a built-in dataset 'sleepstudy'
data("sleepstudy")
#import a built-in dataset 'sleepstudy'
data("sleepstudy")
force(sleepstudy)
#fit the random intercept model, 'REML = F' will allow us see information criteria for the model
intercept_model <- lmerTest::lmer(Reaction ~ Days + (1|Subject),
data = sleepstudy, REML = F)
summary(intercept_model)
int_and_slope_model <- lmerTest::lmer(Reaction ~ Days + (1+Days|Subject),
data = sleepstudy, REML = F)
summary(int_and_slope_model)
View(sleepstudy)
#fit the random intercept model, 'REML = F' will allow us see information criteria for the model
intercept_model <- lmerTest::lmer(Reaction ~ Days + (1|Subject),
data = sleepstudy, REML = F)
summary(intercept_model)
int_and_slope_model <- lmerTest::lmer(Reaction ~ Days + (1+Days|Subject),
data = sleepstudy, REML = F)
summary(int_and_slope_model)
#here are coefficients of the model that only has random intercepts
coef(intercept_model)
#here are coefficients of the model that only has random intercepts
coef(intercept_model)
#here we can see what intercepts and slopes model estimated for every subject
coef(int_and_slope_model)
#Compare models using analysis of variance (will also give AIC and BIC)
anova(intercept_model, int_and_slope_model)
#Check R squared
MuMIn::r.squaredGLMM(intercept_model)
#Compare models using analysis of variance (will also give AIC and BIC)
anova(intercept_model, int_and_slope_model)
#Check R squared
MuMIn::r.squaredGLMM(intercept_model)
MuMIn::r.squaredGLMM(int_and_slope_model)
plot(intercept_model)
plot(int_and_slope_model)
plot(intercept_model)
plot(int_and_slope_model)
#cite packages
citation("base")
citation('lmerTest')
#cite packages
citation("base")
#recall results of assumption check
plot(int_and_slope_model)
#recall the effect sizes and stuff from the best model
summary(int_and_slope_model)
#Read in all the csv files from a folder and bind them together
df <- list.files(path = "EmotionalStroopTask", pattern = "*.csv", full.names = T) %>%
map_dfr(read_csv)
#Read in all the csv files from a folder and bind them together
df <- list.files(path = "logfiles", pattern = "*.csv", full.names = T) %>%
map_dfr(read_csv)
View(df)
View(df)
#Mixed-effect linear model:
Hypno_Sissy_Model <- lmerTest::lmer(Reaction_time ~ Emotional + (1|ID),
data = df, REML = F)
Hypno_Sissy_Model
summary(Hypno_Sissy_Model)
plot(Hypno_Sissy_Model)
#Mixed-effect linear model:
Hypno_Sissy_Model <- lmerTest::lmer(Reaction_time ~ Sentiment + (1|ID),
data = df, REML = F)
plot(Hypno_Sissy_Model)
summary(Hypno_Sissy_Model)
MuMIn::r.squaredGLMM(Hypno_Sissy_Model)
plot(Hypno_Sissy_Model)
#Mixed-effect linear model:
Hypno_Sissy_Model <- lmerTest::lmer(Reaction_time ~ Sentiment + (1|ID),
data = df, REML = F)
summary(Hypno_Sissy_Model)
MuMIn::r.squaredGLMM(Hypno_Sissy_Model)
plot(Hypno_Sissy_Model)
Hypno_Sissy_Model <- lmerTest::lmer(Reaction_time ~ color + (1|ID),
data = df, REML = F)
Hypno_Sissy_Model <- lmerTest::lmer(Reaction_time ~ Color + (1|ID),
data = df, REML = F)
summary(Hypno_Sissy_Model2)

---
title: "MRC exercise"
author: "KT & EK"
date: "10/12/2021"
output: html_document
---

# What aspects of words drive reading times?  

In this exercise we will investigate different properties of words and whether they affect word-by-word reading times. For this purpose we will rely on logfiles from your reading experiment and the MRC Psycholinguistics Database (http://websites.psychology.uwa.edu.au/school/MRCDatabase/uwa_mrc.htm). 

The database is a kind of dictionary that has 120392 words scored on 13 different dimensions from the number of syllables to word frequencies, ratings of the words' familiarity and concreteness to their age of acquisition.       

The database is designed to be used from the browser website. This works well for individual words, but is very inefficient when you are interested in many words making up a full text. Therefore we will import the database as a textfile into RStudio and use a bit of code to quiry the relevant data.    

Make sure that you have downloaded the database (called "MRC_database.csv"") from Blackboard to your working directory. Also you should choose a logfile from the reading experiment that you want to work with.

```{r}
# first we load packages (e.g. tidyverse)
pacman::p_load(tidyverse, stringr)
```

# Import the database into R

I prepared a version of the database in regular .csv so you can import it like you would usually import data 

```{r}
# import the database
mrc <- read_csv("MRC_database.csv")

# import a log file from your reading experiment
log <- read_csv("sample_logfile.csv")

```

# Get properties of words in reading logfile from database 

In order to look up properties of words in the text you used in your reading experiment, we need to make R compare the column with words in your logfile with the column with words in the database. In order for that to happen, we need to make sure that these columns have the same name. In the database the column is called "word". It would therefore be a good idea to rename the corresponding column in your logfile with the same name if it is not already called "word". 

You can modify and use this code to change the name of the word column in your reading experiment logfile: 

df <- df %>% rename(word = old_column_name)

```{r}
# change name of word column in logfile
log <- log %>% 
  rename(word = word)

```

# Convert letters to capitals 

Since all words in the database are written in uppercase, we need to change the words in your reading experiment logfile to uppercase - otherwise R cannot recognize the words as being the same. You can use the command str_to_upper() to convert the whole word column to uppercase e.g. in a mutate() statement.

```{r}
# convert word column to uppercase
log <- log %>% 
  mutate(word = str_to_upper(word))

```

# Look up words from your text in the database

Now we are ready to get properties of the words in our text from the database. We do this by merging the two dataframes, the reading experiment logfile and the database using the word column as "key". This means that R will search the database for those words that are identical between the text and the database and add columns for these words to our dataframe. For this we use the function 

log <- log %>% inner_join(mrc)

(Again the example assumes that your logfile is called log and the database is called mrc - you might need to make changes to the code so that it matches with your way of naming these data frames).

```{r}
# add word properties from the database to your reading experiment data frame
df <- log %>% 
  inner_join(mrc)

```

# Fix duplicates

The dataset has issues probably generated by the fact that there are several encodings of the same words in the MRC database. We remove all duplicates by removing rows where the previous word is equal to the current word (with the assumption that this is a normal text).

```{r}

nodup_df <- df %>% 
  mutate(
    var = if_else(is.na(lag(word)), 
                  TRUE, 
                  lag(word) != word)
    ) %>% 
  filter(var)

```


# Explore relations

Now we can explore the relation between reading times and varies aspects of words using correlation and scatter plots. Perform at least two exploratory tests between reading time and a selected variable from the database. In addition, try to compare length of words and word frequency. What do you find? and why?

```{r}

# Check normality
round(pastecs::stat.desc(cbind(nodup_df$rt, nodup_df$nlet, nodup_df$brown_freq), basic = FALSE, norm = TRUE), digits = 2)

# Remove outliers
nodup_df <- nodup_df %>% 
  filter(
    rt > mean(rt)-3*sd(rt) & rt < mean(rt)+3*sd(rt)
  )

# Transform variables if necessary for example for the reading time
nodup_df <- nodup_df %>% 
  mutate(rt_log = log(rt),
         rt_sqrt = sqrt(rt),
         rt_rec = 1/rt)

# Your correlation analyses
nodup_df %>% 
  select(where(is.numeric)) %>% 
  cor() %>% 
  corrplot(method="circle")

# (Advanced) Ask Esben if you want to know why several of these steps have a quite a few theoretical pitfalls

```

# Plot relations

You can use the normal `df %>% ggplot() + aes(reading_time, length_of_word) + geom_point()` to show the scatter plot of different variables. By including `geom_smooth()`, you get a trend line.


```{r}
nodup_df %>% 
  ggplot() +
  aes(rt_log, kf_freq) +
  geom_point() +
  geom_smooth()

```

*Extra*: See if you can make the trend line (`geom_smooth`) linear and without the grey bars.

```{r}
nodup_df %>% 
  ggplot() +
  aes(rt_log, kf_freq) +
  geom_point() +
  geom_smooth(method="lm", se=F)

```



# Advanced

You could be interested in investigating the same relations but divide your analysis for different "parts of speech". Maybe the relation between ver frequent function words like "a", "the", "that", "and" is different from juicy content words like nouns, adjectives and verbs? 

If you want to explore this additional dimension, you can load the package tidytext and from this get "parts-of-speech-tagging"

Ones the package is loaded you can inner_join() with parts_of_speech. Notice, however, that this dictionary needs the word column to be in lower case - so you have to convert it back :-) 

This operation gives you an additional factor column that codes each word for its category. Now you can redo some of your analyses looking at the relations for individual parts of speech - cool!

(see https://m-clark.github.io/text-analysis-with-R/part-of-speech-tagging.html#basic-idea-1 for more guidance)

```{r}
# load the tidytext package and add the pos (parts_of_speech) column to your dataframe
# plot relations for the different parts of speech and run some analyses
pacman::p_load(tidytext)

nodup_df %>% 
  mutate(word = str_to_lower(word)) %>% 
  inner_join(tidytext::parts_of_speech) %>% 
  drop_na(pos) %>% 
  ggplot() +
  aes(rt_log, kf_freq, color = pos) +
  geom_point() +
  geom_smooth(method="lm", se=F) +
  facet_wrap(~pos) +
  theme(legend.position="none")

```


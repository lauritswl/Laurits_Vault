Written by: Laurits Lyngbæk
Source of information:
Association links: 
Tags: #📑ChildNode 
___
##  Word Embeddings
Word embeddings is a representation of a word (a vector in a multidimensional space), that can be used to express the relation between two words (their distance between each other).
**It's in other words a way to quantify “semantics”**

### Use in CogSci setting:
Word embeddings can be used for a proximation of “meaning” representation in the human mind
(e.g. spreading activation/associative neural network theory)






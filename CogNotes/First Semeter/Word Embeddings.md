Written by: Laurits LyngbÃ¦k
Source of information:
Association links: 
Tags: #ğŸ“‘ChildNode 
___
##  Word Embeddings
Word embeddings is a representation of a word (a vector in a multidimensional space), that can be used to express the relation between two words (their distance between each other).
**It's in other words a way to quantify â€œsemanticsâ€**

### Use in CogSci setting:
Word embeddings can be used for a proximation of â€œmeaningâ€ representation in the human mind
(e.g. spreading activation/associative neural network theory)






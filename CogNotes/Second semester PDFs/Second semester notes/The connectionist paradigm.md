Written by: Laurits Lyngbæk
Source of information:
Association links:
Tags: 
___
# Connectionism
The second paradigm in Cognitive Science.
## General description of connectionism
Parallel Distributed Processing (PDP)
**Neural networks**: 
Interconnected neurons distributed across the brain. 
Subject to Hebbian plasticity.
Finite number of neurons.

**Easy Definition**: Simulated network of neurons which learns to represent the world in its network structure.
### Artificial neural netoworks
Galaxy vs object recognition - an artificial neural network will often learn to recognize an object in a situation, when the object is moved from that situation, it has a harder time recognize it.




#### Network of McCulloch-Pitts neurons
![Connectionist approach](https://upload.wikimedia.org/wikipedia/commons/thumb/e/e4/Artificial_neural_network.svg/1200px-Artificial_neural_network.svg.png)
![[Pasted image 20220615164247.png]]
![[Pasted image 20220615164255.png]]
![[Pasted image 20220615164314.png]]
## Relation to [[Computational theory of mind (CTM)|computationalism]]
To have a system that is **implementation-independent**, you can only pass the **T2 Turing test**. But if you can only pass a **T2 Turing test**, your system will not have **intrinsic semantics**.

To get **semantics** you must pass at least a **T3 Turing test**. But then your system won’t be **implementation independent.**

If computation is implementation-independent, systematically interpretable symbol manipulation, **no computational model will ever be truly cognitive**



## The symbol-grounding problem
Representational level of symbols 
- Representational work is done by the attractors in activation space 
- E.g. one attractor for mines and one attractor for reefs 
- Semantics is distributed across the network 
- Semantics emerges as a global network property 
- UNITS IN A NETWORK EMBODY FORMAL RULES OF COMPUTATION


- Q: how does connenctionism solve the symbol grounding problem?  

- A: (Lau): m. from the attractors in the state space, meaning can be inferred, but see the critique in the slides from week 17, that these are set by the programmers' intentions and are thus not intrinsic to the system



**Grounding from Chalmers (1992):**
*Casual grounding*:
Proponents of causal grounding hold that there can be no reference in a vacuum, and true representation must be grounded in sensorimotor interaction with the environment. For instance, if the representation **DOG** is triggered by the presence of **actual dogs** in the environment, then one might fairly claim that the representation really does **have dog as its external referent**


Internal grounding: 
Is ensuring that our representations have sufficient internal structure for them to carry intrinsic content.
If we use representational vehicles that are not primitive tokens, but instead possess rich internal pattern, the problem of intrinsic content might be solved.
The internal structure of the representation will be relatively similar to the structure of representations of objects to which this example is semantically similar in relevant respects, and quite different from the structure of semantically different objects.



The attractors (what creates the structural semantics) are defined by the programmers. Therefore they are not *intrinsic* semantics, as they are not intrinsically grounded in system function, but imposed by a human cognition.
